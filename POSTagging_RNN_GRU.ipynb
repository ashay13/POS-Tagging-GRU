{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Activation, Dense, Dropout, RepeatVector, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fedata = open(\"treebank_sents.txt\", \"w\")\n",
    "ffdata = open(\"treebank_poss.txt\", \"w\")\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "for sent in brown.tagged_sents():\n",
    "    words, poss = [], []\n",
    "    for word, pos in sent:\n",
    "        if len(tokenizer.tokenize(word)) != 1 :\n",
    "            continue\n",
    "        words.append(word)\n",
    "        poss.append(pos)\n",
    "    fedata.write(\"{:s}\".format(\" \".join(words)))\n",
    "    fedata.write(\"\\n\")\n",
    "    ffdata.write(\"{:s}\".format(\" \".join(poss)))\n",
    "    ffdata.write(\"\\n\")\n",
    "\n",
    "fedata.close()\n",
    "ffdata.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open(\"treebank_poss.txt\", \"rb\")\n",
    "diction = {}\n",
    "for line in fin:\n",
    "    line=str(line.decode('ascii',\"ignore\").strip())\n",
    "    line.strip().split(\"\\t\")\n",
    "    sentence = line.strip().split(\"\\t\")\n",
    "    for tag in sentence:\n",
    "        #print(tag)\n",
    "        if tag in diction:\n",
    "            diction[tag] += 1\n",
    "        else:\n",
    "            diction[tag] = 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Words - 41845\n",
      " Max Sentence - 167\n",
      " TotalWords - 57340\n",
      "Unique tags - 339\n",
      " Max tag - 167\n",
      " Totaltags - 57340\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_sentences(filename):\n",
    "    word_freqs = collections.Counter()\n",
    "    num_recs, maxlen = 0, 0\n",
    "    fin = open(filename, \"rb\")\n",
    "    for line in fin:\n",
    "        words = line.strip().lower().split()\n",
    "        for word in words:\n",
    "            word_freqs[word] += 1\n",
    "        if len(words) > maxlen:\n",
    "            maxlen = len(words)\n",
    "        num_recs += 1\n",
    "    fin.close()\n",
    "    return word_freqs, maxlen, num_recs\n",
    "\n",
    "s_wordfreqs, s_maxlen, s_numrecs = parse_sentences(\"treebank_sents.txt\")\n",
    "t_wordfreqs, t_maxlen, t_numrecs = parse_sentences(\"treebank_poss.txt\")\n",
    "print(\"Unique Words - {0}\\n Max Sentence - {1}\\n TotalWords - {2}\\nUnique tags - {3}\\n Max tag - {4}\\n Totaltags - {5}\\n\".format(len(s_wordfreqs), s_maxlen, s_numrecs, len(t_wordfreqs), t_maxlen, t_numrecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQLEN = 170\n",
    "S_MAX_FEATURES = 55000\n",
    "T_MAX_FEATURES = 340"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_vocabsize = min(len(s_wordfreqs), S_MAX_FEATURES) + 2\n",
    "s_word2index = {x[0]:i+2 for i, x in enumerate(s_wordfreqs.most_common(S_MAX_FEATURES))}\n",
    "s_word2index[\"PAD\"] = 0\n",
    "s_word2index[\"UNK\"] = 1\n",
    "s_index2word = {v:k for k, v in s_word2index.items()}\n",
    "\n",
    "t_vocabsize = len(t_wordfreqs) + 1\n",
    "t_word2index = {x[0]:i for i, x in enumerate(t_wordfreqs.most_common(T_MAX_FEATURES))}\n",
    "t_word2index[\"PAD\"] = 0\n",
    "t_index2word = {v:k for k, v in t_word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tensor(filename, numrecs, word2index, maxlen,make_categorical=False, num_classes=0):\n",
    "    data = np.empty((s_numrecs, ), dtype=list)\n",
    "    fin = open(filename, \"rb\")\n",
    "    i = 0\n",
    "    for line in fin:\n",
    "        wids = []\n",
    "        for word in line.strip().lower().split():\n",
    "            if word in word2index:\n",
    "                wids.append(word2index[word])\n",
    "            else:\n",
    "                wids.append(word2index[\"UNK\"])\n",
    "        if make_categorical:\n",
    "            data[i] = np_utils.to_categorical(wids,num_classes=num_classes)\n",
    "        else:\n",
    "            data[i] = wids\n",
    "        i += 1\n",
    "    fin.close()\n",
    "    pdata = sequence.pad_sequences(data, maxlen=maxlen)\n",
    "    return pdata\n",
    "\n",
    "X = build_tensor((\"treebank_sents.txt\"),s_numrecs, s_word2index, MAX_SEQLEN)\n",
    "Y = build_tensor((\"treebank_poss.txt\"),t_numrecs, t_word2index, MAX_SEQLEN, True, t_vocabsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 128\n",
    "HIDDEN_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 1\n",
    "model = Sequential()\n",
    "model.add(Embedding(s_vocabsize, EMBED_SIZE,input_length=MAX_SEQLEN))\n",
    "#model.add(SpatialDropout1D(Dropout(0.2)))\n",
    "model.add(GRU(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(RepeatVector(MAX_SEQLEN))\n",
    "model.add(GRU(HIDDEN_SIZE, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(t_vocabsize)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45872 samples, validate on 11468 samples\n",
      "Epoch 1/1\n",
      "45872/45872 [==============================] - 1408s 31ms/step - loss: 0.3438 - acc: 0.8977 - val_loss: 0.3393 - val_acc: 0.9136\n",
      "11468/11468 [==============================] - 70s 6ms/step\n",
      "Test score: 0.339, accuracy: 0.914\n"
     ]
    }
   ],
   "source": [
    "model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,validation_data=[Xtest, Ytest])\n",
    "score, acc = model.evaluate(Xtest, Ytest, batch_size=BATCH_SIZE)\n",
    "print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
